# âš–ï¸ Lawyer RAG - Legal AI Chatbot

A Retrieval-Augmented Generation (RAG) based Legal AI Chatbot that helps users interact with legal documents, laws, and case files using advanced language models.

## ğŸ“‹ Overview

Lawyer RAG is an intelligent legal assistant that combines the power of vector databases, embeddings, and large language models (LLMs) to provide accurate answers to legal queries. The system processes legal documents and case files, stores them in vector databases, and uses RAG techniques to retrieve relevant context before generating responses.

## âœ¨ Key Features

- **ğŸ“‚ Document Processing**: Upload and process PDF documents for laws and case files
- **ğŸ¤– Multiple AI Models**: Support for Google Gemini and Ollama models
- **ğŸ” Intelligent Retrieval**: FAISS-based vector search for relevant document retrieval
- **ğŸ’¬ Interactive Chat Interface**: User-friendly Streamlit-based chat UI
- **ğŸš€ REST API**: FastAPI backend for programmatic access
- **ğŸ³ Docker Support**: Containerized deployment for easy setup
- **ğŸ§¹ Database Management**: Built-in cleanup functionality for vector databases

## ğŸ› ï¸ Technology Stack

- **Backend Framework**: FastAPI
- **Frontend**: Streamlit
- **LLM Integration**: 
  - Google Gemini (gemini-2.0-flash)
  - Ollama (multiple models support)
- **Vector Database**: FAISS
- **Embeddings**: Google Generative AI Embeddings (embedding-001)
- **Document Processing**: PyPDF2, LangChain
- **Language**: Python 3.10+

## ğŸ“¦ Installation

### Prerequisites

- Python 3.10 or higher
- Google API Key (for Gemini model)
- Docker (optional, for containerized deployment)

### Local Setup

1. **Clone the repository**:
   ```bash
   git clone https://github.com/sanjithwoxsen/Lawyer_RAG.git
   cd Lawyer_RAG
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure environment variables**:
   Create a `.env` file in the root directory:
   ```env
   GOOGLE_API_KEY=your_google_api_key_here
   ```

## ğŸš€ Usage

### Running the FastAPI Server

Start the FastAPI backend server:

```bash
uvicorn fastapi_server:app --host 0.0.0.0 --port 8000
```

The API will be available at `http://localhost:8000`

### Running the Streamlit UI

In a separate terminal, launch the Streamlit interface:

```bash
streamlit run main.py
```

The web interface will open at `http://localhost:8501`

### Using Docker

Build and run the application using Docker:

```bash
# Build the Docker image
docker build -t lawyer-rag .

# Run the container
docker run -p 8000:8000 lawyer-rag
```

Note: The Dockerfile currently runs only the FastAPI server. To run the Streamlit UI, you'll need to run it separately or modify the Dockerfile.

## ğŸ“š API Endpoints

### 1. List Available Models
```http
GET /list_models/
```
Returns a list of available Ollama and Gemini models.

### 2. Upload Law Documents
```http
POST /upload_law/
```
Upload legal/law documents (PDFs) for processing.

**Parameters**: 
- `law_files`: List of PDF files

### 3. Upload Case Documents
```http
POST /upload_case/
```
Upload case-related documents (PDFs) for processing.

**Parameters**: 
- `case_files`: List of PDF files

### 4. Query the AI
```http
POST /query/
```
Ask questions to the AI based on uploaded documents.

**Request Body**:
```json
{
  "question": "Your legal question here",
  "model_choice": "Gemini Pro",
  "ollama_model": "optional_ollama_model_name"
}
```

### 5. Cleanup Database
```http
DELETE /cleanup/
```
Removes all stored documents from the vector database.

## ğŸ“ Project Structure

```
Lawyer_RAG/
â”œâ”€â”€ fastapi_server.py          # FastAPI application entry point
â”œâ”€â”€ main.py                     # Streamlit UI application
â”œâ”€â”€ main2.py                    # Alternative main file
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Dockerfile                  # Docker configuration
â”œâ”€â”€ .env                        # Environment variables (not in repo)
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ fastapi/
â”‚   â”‚   â”œâ”€â”€ api/               # API route handlers
â”‚   â”‚   â”‚   â”œâ”€â”€ upload_law.py
â”‚   â”‚   â”‚   â”œâ”€â”€ upload_case.py
â”‚   â”‚   â”‚   â”œâ”€â”€ query.py
â”‚   â”‚   â”‚   â”œâ”€â”€ list_models.py
â”‚   â”‚   â”‚   â””â”€â”€ cleanup.py
â”‚   â”‚   â”œâ”€â”€ services/          # Business logic
â”‚   â”‚   â”‚   â”œâ”€â”€ document_handler.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model_handler.py
â”‚   â”‚   â”‚   â””â”€â”€ cleanup_handler.py
â”‚   â”‚   â””â”€â”€ schemas/           # Pydantic models
â”‚   â”‚       â”œâ”€â”€ query.py
â”‚   â”‚       â”œâ”€â”€ cleanup.py
â”‚   â”‚       â””â”€â”€ Ollama_external_url.py
â”‚   â”œâ”€â”€ workflow/
â”‚   â”‚   â”œâ”€â”€ document/          # Document processing
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”‚   â”‚   â”œâ”€â”€ vector_db.py
â”‚   â”‚   â”‚   â”œâ”€â”€ datapreprocess.py
â”‚   â”‚   â”‚   â””â”€â”€ cleanup.py
â”‚   â”‚   â”œâ”€â”€ llm/               # LLM integrations
â”‚   â”‚   â”‚   â”œâ”€â”€ gemini.py
â”‚   â”‚   â”‚   â””â”€â”€ ollama_llms.py
â”‚   â”‚   â””â”€â”€ retrieval/         # Document retrieval
â”‚   â”‚       â””â”€â”€ vector_retriever.py
â”‚   â””â”€â”€ utils/                 # Utility functions
â”‚       â”œâ”€â”€ gemini_config.py
â”‚       â”œâ”€â”€ log_decorator.py
â”‚       â”œâ”€â”€ interaction_logger.py
â”‚       â””â”€â”€ docker_utils.py
â””â”€â”€ logs/                      # Application logs
```

## ğŸ¯ How It Works

1. **Document Upload**: Users upload legal documents (laws and case files) through the Streamlit UI or API
2. **Processing**: Documents are parsed, split into chunks, and converted to embeddings
3. **Storage**: Embeddings are stored in FAISS vector databases (separate databases for laws and cases)
4. **Query**: When a user asks a question, the system:
   - Converts the question to an embedding
   - Retrieves relevant document chunks from the vector databases
   - Passes the context to the selected LLM (Gemini or Ollama)
   - Returns a comprehensive answer based on the retrieved context

## ğŸ”§ Configuration

### Google Gemini API

To use Google Gemini models, you need to:
1. Get an API key from [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Add it to your `.env` file:
   ```env
   GOOGLE_API_KEY=your_api_key_here
   ```

### Ollama Models

To use Ollama models:
1. Install [Ollama](https://ollama.ai/) on your system
2. Pull the desired models (e.g., `ollama pull llama2`)
3. The application will automatically detect available Ollama models

## ğŸ“ Development

### Running Tests

Currently, this project doesn't include a test suite. To validate functionality:

1. Start the FastAPI server
2. Use the Streamlit UI or API endpoints to test features
3. Check logs in the `logs/` directory for any errors

### Adding New Features

The modular structure makes it easy to extend functionality:
- Add new API endpoints in `modules/fastapi/api/`
- Implement business logic in `modules/fastapi/services/`
- Add new LLM integrations in `modules/workflow/llm/`

## ğŸ‘¥ Credits

Developed by students of **Woxsen University**.

## ğŸ“„ License

This project's license information is not specified. Please contact the repository owner for licensing details.

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues or pull requests.

## âš ï¸ Disclaimer

This is an AI-powered tool designed to assist with legal information retrieval. It should not be considered as legal advice. Always consult with qualified legal professionals for legal matters.

---

**Note**: Make sure to keep your `.env` file and API keys secure and never commit them to version control.
